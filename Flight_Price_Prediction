# 1) Imports and configuration

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader

RANDOM_STATE = 42
SUBSAMPLE_N = 100000  

np.random.seed(RANDOM_STATE)
torch.manual_seed(RANDOM_STATE)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# 2) Load and prepare dataset

DATA_PATH = r"C:\Users\user\OneDrive\Desktop\Sunway Uni\Computational Intlligence\Clean_Dataset.csv"
df = pd.read_csv(DATA_PATH)


if "Unnamed: 0" in df.columns:
    df = df.drop(columns=["Unnamed: 0"])

print("Columns in raw dataset:", df.columns.tolist())

TARGET_COL = "price"
if TARGET_COL not in df.columns:
    raise ValueError(f"Target column '{TARGET_COL}' not found in dataset!")


if len(df) > SUBSAMPLE_N:
    df = df.sample(n=SUBSAMPLE_N, random_state=RANDOM_STATE).reset_index(drop=True)
    print(f"\nSubsampled dataset to {len(df)} rows for training.")
else:
    print(f"\nUsing full dataset with {len(df)} rows.")

categorical_cols = [
    "airline",
    "source_city",
    "departure_time",
    "stops",
    "arrival_time",
    "destination_city",
    "class",
]

numeric_cols = ["duration", "days_left"]

X = df[categorical_cols + numeric_cols]
y = df[TARGET_COL].values

for col in categorical_cols + numeric_cols:
    if col not in X.columns:
        raise ValueError(f"Expected column '{col}' not found in features!")

print("\nNumber of samples used:", len(X))

# 3) Train / validation / test split (70 / 15 / 15)
# First split: 70% train, 30% temp (val + test)

X_train, X_temp, y_train, y_temp = train_test_split(
    X,
    y,
    test_size=0.30,  # 30% held out for val+test
    random_state=RANDOM_STATE,
    shuffle=True,
)

# Second split: 30% temp into 15% val and 15% test (0.30 * 0.5 = 0.15)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp,
    y_temp,
    test_size=0.50,  # half of temp â†’ 15% of original
    random_state=RANDOM_STATE,
    shuffle=True,
)

n_total = len(X)
print("\nSplit sizes:")
print(f"  Train: {len(X_train)} ({len(X_train)/n_total*100:.1f}%)")
print(f"  Val  : {len(X_val)} ({len(X_val)/n_total*100:.1f}%)")
print(f"  Test : {len(X_test)} ({len(X_test)/n_total*100:.1f}%)")


# 4) Preprocessing pipeline (dense float32)

preprocessor = ColumnTransformer(
    transformers=[
        (
            "cat",
            OneHotEncoder(
                handle_unknown="ignore",
                sparse_output=False,   # output dense array
                dtype=np.float32
            ),
            categorical_cols,
        ),
        (
            "num",
            StandardScaler(),
            numeric_cols,
        ),
    ]
)

X_train_proc = preprocessor.fit_transform(X_train)
X_val_proc   = preprocessor.transform(X_val)
X_test_proc  = preprocessor.transform(X_test)

X_train_proc = X_train_proc.astype(np.float32)
X_val_proc   = X_val_proc.astype(np.float32)
X_test_proc  = X_test_proc.astype(np.float32)

input_dim = X_train_proc.shape[1]
print(f"\nPreprocessed feature dimension: {input_dim}")

# 5) Baseline models

def evaluate_regressor(name, model, X_tr, y_tr, X_te, y_te):
    """Fit model, predict on test set, print MAE and RMSE."""
    model.fit(X_tr, y_tr)
    preds = model.predict(X_te)
    mae = mean_absolute_error(y_te, preds)
    rmse = np.sqrt(mean_squared_error(y_te, preds))
    print(f"[{name}]  MAE: {mae:.2f}   RMSE: {rmse:.2f}")
    return mae, rmse

print("\n=== Baseline models ===")

lr_model = LinearRegression(n_jobs=-1)
lr_mae, lr_rmse = evaluate_regressor(
    "Linear Regression", lr_model, X_train_proc, y_train, X_test_proc, y_test
)

rf_model = RandomForestRegressor(
    n_estimators=200,
    random_state=RANDOM_STATE,
    n_jobs=-1,
    max_depth=None,
)
rf_mae, rf_rmse = evaluate_regressor(
    "Random Forest", rf_model, X_train_proc, y_train, X_test_proc, y_test
)


# 6) PyTorch neural network

class PriceNet(nn.Module):
    """Dense neural network for flight price regression."""
    def __init__(self, input_dim: int):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Dropout(0.30),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)  # linear output for regression
        )

    def forward(self, x):
        return self.net(x)


model = PriceNet(input_dim).to(device)
print("\n=== Neural network architecture (PyTorch) ===")
print(model)

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# 7) Create DataLoaders

BATCH_SIZE = 256

train_dataset = TensorDataset(
    torch.from_numpy(X_train_proc).float(),
    torch.from_numpy(y_train).float().view(-1, 1)
)
val_dataset = TensorDataset(
    torch.from_numpy(X_val_proc).float(),
    torch.from_numpy(y_val).float().view(-1, 1)
)
test_dataset = TensorDataset(
    torch.from_numpy(X_test_proc).float(),
    torch.from_numpy(y_test).float().view(-1, 1)
)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# 8) Training loop with early stopping

EPOCHS = 50
PATIENCE = 5

best_val_loss = float("inf")
best_state = None
epochs_no_improve = 0

print("\n=== Training neural network (PyTorch) ===")
for epoch in range(1, EPOCHS + 1):
    # train
    model.train()
    train_losses = []
    for xb, yb in train_loader:
        xb = xb.to(device)
        yb = yb.to(device)

        optimizer.zero_grad()
        preds = model(xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())

    # validation
    model.eval()
    val_losses = []
    with torch.no_grad():
        for xb, yb in val_loader:
            xb = xb.to(device)
            yb = yb.to(device)
            preds = model(xb)
            loss = criterion(preds, yb)
            val_losses.append(loss.item())

    avg_train = float(np.mean(train_losses))
    avg_val = float(np.mean(val_losses))
    print(f"Epoch {epoch:02d} | Train loss: {avg_train:.4f} | Val loss: {avg_val:.4f}")

    # Early stopping
    if avg_val < best_val_loss - 1e-4:
        best_val_loss = avg_val
        best_state = model.state_dict()
        epochs_no_improve = 0
    else:
        epochs_no_improve += 1
        if epochs_no_improve >= PATIENCE:
            print(f"Early stopping triggered at epoch {epoch}.")
            break

if best_state is not None:
    model.load_state_dict(best_state)

# 9) Evaluation on test set

print("\n=== Neural network test performance ===")
model.eval()
all_preds = []
all_true = []

with torch.no_grad():
    for xb, yb in test_loader:
        xb = xb.to(device)
        preds = model(xb).cpu().numpy().flatten()
        all_preds.append(preds)
        all_true.append(yb.numpy().flatten())

all_preds = np.concatenate(all_preds)
all_true = np.concatenate(all_true)

nn_mae = mean_absolute_error(all_true, all_preds)
nn_rmse = np.sqrt(mean_squared_error(all_true, all_preds))
print(f"[Neural Network (PyTorch)] MAE: {nn_mae:.2f}   RMSE: {nn_rmse:.2f}")

print("\nSuccessful.")

import matplotlib.pyplot as plt

models = ["Linear", "Random Forest", "NN (PyTorch)"]
maes = [lr_mae, rf_mae, nn_mae]

plt.figure()
plt.bar(models, maes)
plt.ylabel("MAE")
plt.title("Test MAE Comparison")
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 5))
plt.plot(train_losses, label="Training Loss")
plt.plot(val_losses, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss (MSE)")
plt.title("Training and Validation Loss Across Epochs")
plt.legend()
plt.grid(True)
plt.show()
